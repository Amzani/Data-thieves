{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supermarket Data Thieves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we scraped supermarketcheck.de \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a outer for loop for multiple supermarkets.\n",
    "\n",
    "supermarket_list = [\"aldi-nord\", \"aldi-sued\", \"edeka\", \"kaufland\", \"rewe\", \"lidl\"]\n",
    "\n",
    "for i in range(0,len(supermarket_list)):\n",
    "    \n",
    "    j = supermarket_list[i]\n",
    "    \n",
    "    result_frame = supermarket_scraper(j,True)\n",
    "    \n",
    "    if \"aldi\" not in j:\n",
    "        \n",
    "        globals()['frame_%s' % j] = result_frame\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        globals()['frame_aldi_%s' % i] = result_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a function\n",
    "\n",
    "\n",
    "def supermarket_scraper(supermarket_input,print_input):\n",
    "\n",
    "    ## Scrape supermarket products\n",
    "\n",
    "    # Initialize empty framelist for dataframe and set supermarket\n",
    "    \n",
    "    frame_list = []\n",
    "    supermarket = supermarket_input\n",
    "    prints = print_input\n",
    "    \n",
    "    if prints:\n",
    "        print(\"Current supermarket: \", supermarket)\n",
    "    # Check how many pages for this supermarket\n",
    "    \n",
    "    pageno = 2\n",
    "    base = \"https://www.supermarktcheck.de/\"\n",
    "    url = base + f\"{supermarket}/produkte/page:{pageno}\"\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "    pages = str(soup.find(\"div\", class_=\"col-sm-6 text-center btn-pagination-title\").contents).replace(\"\\\\t\",\"\").replace(\"\\\\nSeite \",\"\".replace(\"\\[\",\"\"))\n",
    "    pattern = r\"[2 von ](\\d+)\"\n",
    "    pagenumbers = int(re.findall(pattern,pages)[0])\n",
    "    \n",
    "    \n",
    "        # Iterate through all pages for the supermarket #pagenumbers+1\n",
    "        \n",
    "    for j in range(1, pagenumbers+1):\n",
    "        \n",
    "        # Scrape soup for each page\n",
    "        \n",
    "        supermarket = supermarket_input\n",
    "        pageno = j\n",
    "        base = \"https://www.supermarktcheck.de/\"\n",
    "        url = base + f\"{supermarket}/produkte/page:{pageno}\"\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        \n",
    "        # Get a list of products per page\n",
    "        \n",
    "        product_list = soup.find(\"div\", class_=\"col-sm-8 col-md-9\").find_all(\"div\", \"supermarketListElement\")\n",
    "        \n",
    "        \n",
    "        # List of items per page\n",
    "        \n",
    "        item_names = soup.find_all(\"a\", class_=\"supermarket\")\n",
    "        \n",
    "        \n",
    "        # Count how many items per page excluding empty items\n",
    "        \n",
    "        counter = 0\n",
    "        for i in product_list:\n",
    "            try:\n",
    "                whatever = i.find(\"div\", class_=\"col-md-5 col-lg-5\").find(\"strong\")\n",
    "                counter += 1\n",
    "            except:\n",
    "                next\n",
    "                \n",
    "                \n",
    "        if prints:\n",
    "            print(\"items on page:\",counter)\n",
    "        \n",
    "        \n",
    "        # For loop for all products on one page\n",
    "        \n",
    "        for i in range(0,counter):\n",
    "            \n",
    "            # Initialize a list as empty dataframe row\n",
    "            \n",
    "            frame_element = []\n",
    "            \n",
    "            \n",
    "            # Get the item names\n",
    "            \n",
    "            supermarket_list = item_names[i].contents[0]\n",
    "            frame_element.append(supermarket_list)\n",
    "\n",
    "\n",
    "            # Get the item prices\n",
    "\n",
    "            price = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "            price = price[i].p.strong.contents[0]\n",
    "            frame_element.append(price)\n",
    "\n",
    "\n",
    "            # Get price per unit and unit\n",
    "\n",
    "            price_per = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "            price_per_item_price = price_per[i].span.contents[0]\n",
    "            price_per_item_i = price_per[i].small.contents[0]\n",
    "            frame_element.append(price_per_item_price)\n",
    "            frame_element.append(price_per_item_i)\n",
    "\n",
    "\n",
    "            # Get the packet size\n",
    "\n",
    "            packet_size = soup.find_all(\"div\", class_=\"col-md-7 col-lg-7\")\n",
    "            packet_size = packet_size[i].find_all(\"p\")[1].contents[0]\n",
    "            frame_element.append(packet_size)\n",
    "\n",
    "\n",
    "            # Append the list to the frame list\n",
    "\n",
    "            frame_list.append(frame_element)\n",
    "\n",
    "        if prints:\n",
    "            print(\"page: \", j)\n",
    "        \n",
    "    # Transform the frame list into a dataframe\n",
    "    \n",
    "    frame = pd.DataFrame(frame_list,columns=[\"Name\",\"Price\",\"Price per\", \"Unit\", \"Packet Size\"])\n",
    "    frame[\"Supermarket\"] = supermarket\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape supermarket products\n",
    "\n",
    "\n",
    "# Initialize empty framelist for dataframe and set supermarket\n",
    "\n",
    "frame_list = []\n",
    "supermarket = \"edeka\"\n",
    "prints = False\n",
    "\n",
    "# Check how many pages for this supermarket\n",
    "\n",
    "pageno = 2\n",
    "base = \"https://www.supermarktcheck.de/\"\n",
    "url = base + f\"{supermarket}/produkte/page:{pageno}\"\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "pages = str(soup.find(\"div\", class_=\"col-sm-6 text-center btn-pagination-title\").contents).replace(\"\\\\t\",\"\").replace(\"\\\\nSeite \",\"\".replace(\"\\[\",\"\"))\n",
    "pattern = r\"[2 von ](\\d+)\"\n",
    "pagenumbers = int(re.findall(pattern,pages)[0])\n",
    "\n",
    "\n",
    "# Iterate through all pages for the supermarket\n",
    "\n",
    "for j in range(1,pagenumbers+1):\n",
    "    \n",
    "    # Scrape soup for each page\n",
    "    \n",
    "    supermarket = \"edeka\"\n",
    "    pageno = j\n",
    "    base = \"https://www.supermarktcheck.de/\"\n",
    "    url = base + f\"{supermarket}/produkte/page:{pageno}\"\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # Get a list of products per page\n",
    "    \n",
    "    product_list = soup.find(\"div\", class_=\"col-sm-8 col-md-9\").find_all(\"div\", \"supermarketListElement\")\n",
    "    \n",
    "    \n",
    "    # List of items per page\n",
    "    \n",
    "    item_names = soup.find_all(\"a\", class_=\"supermarket\")\n",
    "    \n",
    "    \n",
    "    # Count how many items per page excluding empty items\n",
    "    \n",
    "    counter = 0\n",
    "    for i in product_list:\n",
    "        try:\n",
    "            whatever = i.find(\"div\", class_=\"col-md-5 col-lg-5\").find(\"strong\")\n",
    "            counter += 1\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "            \n",
    "    if prints:\n",
    "        print(\"items on page:\",counter)\n",
    "    \n",
    "    \n",
    "    # For loop for all products on one page\n",
    "    \n",
    "    for i in range(0,counter):\n",
    "        \n",
    "        # Initialize a list as empty dataframe row\n",
    "        \n",
    "        frame_element = []\n",
    "        \n",
    "        \n",
    "        # Get the item names\n",
    "        \n",
    "        supermarket_list = item_names[i].contents[0]\n",
    "        frame_element.append(supermarket_list)\n",
    "        \n",
    "        \n",
    "        # Get the item prices\n",
    "        \n",
    "        price = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "        price = price[i].p.strong.contents[0]\n",
    "        frame_element.append(price)\n",
    "        \n",
    "        \n",
    "        # Get price per unit and unit\n",
    "        \n",
    "        price_per = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "        price_per_item_price = price_per[i].span.contents[0]\n",
    "        price_per_item_i = price_per[i].small.contents[0]\n",
    "        frame_element.append(price_per_item_price)\n",
    "        frame_element.append(price_per_item_i)\n",
    "        \n",
    "        \n",
    "        # Get the packet size\n",
    "        \n",
    "        packet_size = soup.find_all(\"div\", class_=\"col-md-7 col-lg-7\")\n",
    "        packet_size = packet_size[i].find_all(\"p\")[1].contents[0]\n",
    "        frame_element.append(packet_size)\n",
    "        \n",
    "        \n",
    "        # Append the list to the frame list\n",
    "        \n",
    "        frame_list.append(frame_element)\n",
    "    \n",
    "    if prints:\n",
    "        print(\"page: \", j)\n",
    "        \n",
    "# Transform the frame list into a dataframe\n",
    "\n",
    "frame = pd.DataFrame(frame_list,columns=[\"Name\",\"Price\",\"Price per\", \"Unit\", \"Packet Size\"])\n",
    "frame[\"Supermarket\"] = supermarket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the data for Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandlist = list(soup.find(\"ul\", class_=\"list-group\").find_all(\"li\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(full_brand_list).to_csv(\"brandlist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "list_init = [\"7-days\"]\n",
    "\n",
    "while counter != 52:\n",
    "    \n",
    "    # Get last brand on the list and open page\n",
    "    \n",
    "    last_brand = list_init[-1]\n",
    "    print(\"next page:\", last_brand)\n",
    "    url = f\"https://www.supermarktcheck.de/{last_brand}/\"\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # Get all brands displayed on page and append to list\n",
    "    \n",
    "    brandlist = list(soup.find(\"ul\", class_=\"list-group\").find_all(\"li\"))\n",
    "    \n",
    "    for i in brandlist:\n",
    "        \n",
    "        appender = i.find(\"a\").contents[0]\n",
    "        \n",
    "        if \"Tchibo\" in appender:\n",
    "            appender = str(appender) + \"-1\"\n",
    "            \n",
    "        list_init.append(appender.rstrip().replace(\"ö\",\"oe\").replace(\" \",\"-\").replace(\"ü\",\"ue\").replace(\"&\",\"-\").replace(\"®\",\"\"))\n",
    "        \n",
    "    \n",
    "    # Counter\n",
    "    \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "\n",
    "\n",
    "list_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "import bs4\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selenium scraping of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get category values and plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_list = [\"aldi-nord\", \"aldi-sued\", \"edeka\", \"kaufland\", \"rewe\", \"lidl\"]\n",
    "\n",
    "supermarket = supermarket_list[0]\n",
    "url = f\"https://www.supermarktcheck.de/{supermarket}/sortiment\"\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "d = soup.find_all(\"div\", class_=\"panel-body\")[1].find_all(\"select\", class_=\"form-control\")[1].find_all(\"option\")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = []\n",
    "value_list = []\n",
    "for i in d:\n",
    "    category_list.append(i.contents[0])\n",
    "    stri = str(i.contents[0])\n",
    "    value = str(i).replace('<option value=\"',\"\").replace('\">',\" \").replace(\"</option>\",\"\")\n",
    "    value_list.append(value)\n",
    "\n",
    "digit_pattern = r\"^(\\d+\\w)\"\n",
    "digit_value_list = []\n",
    "for i in value_list:\n",
    "    digiz = re.findall(digit_pattern, i)\n",
    "    digit_value_list.append(digiz)\n",
    "    \n",
    "print(category_list, digit_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first \"all category from both\"\n",
    "\n",
    "category_list = category_list[1:]\n",
    "digit_value_list = digit_value_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selenium detour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = 0\n",
    "#options = Options()\n",
    "#options.add_argument(\"--headless\")\n",
    "#driver = webdriver.Firefox(options=options)\n",
    "driver = webdriver.PhantomJS()\n",
    "driver.get(\"https://www.supermarktcheck.de/aldi-nord/sortiment/\")\n",
    "select = Select(driver.find_element_by_name('cat_id'))\n",
    "\n",
    "try:\n",
    "    select.select_by_value(digit_value_list[1][0])\n",
    "except:\n",
    "    driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "    print(\"scrolled at 1\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    submit_button = driver.find_elements_by_class_name(\"submit\")\n",
    "except:\n",
    "    driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "    print(\"scrolled at 2\")\n",
    "    \n",
    "try:\n",
    "    submit_button[0].click()\n",
    "except:\n",
    "    driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "    print(\"scrolled at 3\")\n",
    "\n",
    "html = driver.page_source\n",
    "driver.close()\n",
    "soup = bs4.BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,len(digit_value_list)):\n",
    "    \n",
    "    print(\"product category:\", category_list[k])\n",
    "    \n",
    "    init_frame = pd.DataFrame(columns=['Name','Price','Price per','Unit','Packet Size','Supermarket','Category'])\n",
    "    \n",
    "    for i in range(0,len(supermarket_list)):\n",
    "    \n",
    "        j = supermarket_list[i]\n",
    "        kvalue = digit_value_list[k][0]\n",
    "        \n",
    "        result_frame = supermarket_scraper(j,True,kvalue)\n",
    "        result_frame[\"Category\"] = category_list[k]\n",
    "        \n",
    "        frame_list = [init_frame,result_frame]\n",
    "        init_frame = pd.concat(frame_list)\n",
    "        \n",
    "        if \"aldi\" not in j:\n",
    "            \n",
    "            globals()[f'frame_{j}_{kvalue}'] = result_frame\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            globals()[f'frame_aldi_{i}_{kvalue}'] = result_frame\n",
    "    \n",
    "    init_frame.to_csv(f\"category_{kvalue}_frame.csv\")\n",
    "    globals()[f\"../supermarket_csvs/frame_all_supermarkets_category_{kvalue}\"] = init_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_all_supermarkets_category_21.to_csv(\"csv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Old code to get the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a outer for loop for multiple supermarkets.\n",
    "\n",
    "supermarket_list = [\"aldi-nord\", \"aldi-sued\", \"edeka\", \"kaufland\", \"rewe\", \"lidl\"]\n",
    "\n",
    "for i in range(0,len(supermarket_list)):\n",
    "    \n",
    "    j = supermarket_list[i]\n",
    "    \n",
    "    result_frame = supermarket_scraper(j,True)\n",
    "    \n",
    "    if \"aldi\" not in j:\n",
    "        \n",
    "        globals()['frame_%s' % j] = result_frame\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        globals()['frame_aldi_%s' % i] = result_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a function\n",
    "\n",
    "\n",
    "def supermarket_scraper(supermarket_input,print_input,category_input):\n",
    "\n",
    "    ## Scrape supermarket products\n",
    "\n",
    "    # Initialize empty framelist for dataframe and set supermarket\n",
    "    \n",
    "    frame_list = []\n",
    "    supermarket = supermarket_input\n",
    "    prints = print_input\n",
    "    \n",
    "    if prints:\n",
    "        print(\"Current supermarket: \", supermarket)\n",
    "    # Check how many pages for this supermarket\n",
    "    \n",
    "    pageno = 2\n",
    "    base = \"https://www.supermarktcheck.de/\"\n",
    "    url = base + f\"{supermarket}/produkte/page:{pageno}/cat_id:{category_input}\"\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "    pages = str(soup.find(\"div\", class_=\"col-sm-6 text-center btn-pagination-title\").contents).replace(\"\\\\t\",\"\").replace(\"\\\\nSeite \",\"\".replace(\"\\[\",\"\"))\n",
    "    pattern = r\"[2 von ](\\d+)\"\n",
    "    pagenumbers = int(re.findall(pattern,pages)[0])\n",
    "    \n",
    "    \n",
    "        # Iterate through all pages for the supermarket #pagenumbers+1\n",
    "        \n",
    "    for j in range(1, pagenumbers+1):\n",
    "        \n",
    "        # Scrape soup for each page\n",
    "        \n",
    "        supermarket = supermarket_input\n",
    "        pageno = j\n",
    "        base = \"https://www.supermarktcheck.de/\"\n",
    "        url = base + f\"{supermarket}/produkte/page:{pageno}/cat_id:{category_input}\"\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        \n",
    "        # Get a list of products per page\n",
    "        \n",
    "        product_list = soup.find(\"div\", class_=\"col-sm-8 col-md-9\").find_all(\"div\", \"supermarketListElement\")\n",
    "        \n",
    "        \n",
    "        # List of items per page\n",
    "        \n",
    "        item_names = soup.find_all(\"a\", class_=\"supermarket\")\n",
    "        \n",
    "        \n",
    "        # Count how many items per page excluding empty items\n",
    "        \n",
    "        counter = 0\n",
    "        for i in product_list:\n",
    "            try:\n",
    "                whatever = i.find(\"div\", class_=\"col-md-5 col-lg-5\").find(\"strong\")\n",
    "                counter += 1\n",
    "            except:\n",
    "                next\n",
    "                \n",
    "                \n",
    "        if prints:\n",
    "            print(\"items on page:\",counter)\n",
    "        \n",
    "        \n",
    "        # For loop for all products on one page\n",
    "        \n",
    "        for i in range(0,counter):\n",
    "            \n",
    "            # Initialize a list as empty dataframe row\n",
    "            \n",
    "            frame_element = []\n",
    "            \n",
    "            \n",
    "            # Get the item names\n",
    "            \n",
    "            supermarket_list = item_names[i].contents[0]\n",
    "            frame_element.append(supermarket_list)\n",
    "\n",
    "\n",
    "            # Get the item prices\n",
    "\n",
    "            price = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "            price = price[i].p.strong.contents[0]\n",
    "            frame_element.append(price)\n",
    "\n",
    "\n",
    "            # Get price per unit and unit\n",
    "\n",
    "            price_per = soup.find_all(\"div\", class_=\"col-md-5 col-lg-5\")\n",
    "            price_per_item_price = price_per[i].span.contents[0]\n",
    "            price_per_item_i = price_per[i].small.contents[0]\n",
    "            frame_element.append(price_per_item_price)\n",
    "            frame_element.append(price_per_item_i)\n",
    "\n",
    "\n",
    "            # Get the packet size\n",
    "\n",
    "            packet_size = soup.find_all(\"div\", class_=\"col-md-7 col-lg-7\")\n",
    "            packet_size = packet_size[i].find_all(\"p\")[1].contents[0]\n",
    "            frame_element.append(packet_size)\n",
    "\n",
    "\n",
    "            # Append the list to the frame list\n",
    "\n",
    "            frame_list.append(frame_element)\n",
    "\n",
    "        if prints:\n",
    "            print(\"page: \", j)\n",
    "        \n",
    "    # Transform the frame list into a dataframe\n",
    "    \n",
    "    frame = pd.DataFrame(frame_list,columns=[\"Name\",\"Price\",\"Price per\", \"Unit\", \"Packet Size\"])\n",
    "    frame[\"Supermarket\"] = supermarket\n",
    "    frame[\"Category\"] = category_input\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we cleaned the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_csv = [\"aldi_products.csv\", \"aldisued_products.csv\", \"edeka_products.csv\", \"kaufland_products.csv\", \"rewe_products.csv\", \"lidl_products.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(supermarket_input):\n",
    "    \n",
    "        \n",
    "    df = pd.read_csv(supermarket_input) \n",
    "    \n",
    "    aldi_df = df\n",
    "    \n",
    "    aldi_df[\"Price\"] = aldi_df[\"Price\"].str.replace(\" €\",\"\").str.replace(\",\",\".\")\n",
    "    aldi_df[\"Price per\"] = aldi_df[\"Price per\"].str.replace(\" €\",\"\").str.replace(\",\",\".\")\n",
    "    aldi_df[\"Price\"] = pd.to_numeric(aldi_df['Price'], errors='coerce')\n",
    "    aldi_df[\"Price per\"] = pd.to_numeric(aldi_df['Price per'], errors='coerce')\n",
    "    \n",
    "        \n",
    "    aldi_df[\"1kg Div\"] = np.where(aldi_df[\"Unit\"] == (\"Preis pro 1 KG\"),(aldi_df[\"Price per\"])/10,aldi_df[\"Price per\"])\n",
    "    aldi_df[\"ML Div \"] = np.where(aldi_df[\"Unit\"] == (\"Preis pro 1 Liter\"),(aldi_df[\"Price per\"])/10,aldi_df[\"Price per\"])\n",
    "    aldi_df[\"Div_merged\"] = 0\n",
    "    aldi_df[\"Div_merged\"] = np.where(aldi_df[\"Unit\"].str.contains(\"G\"), aldi_df[\"1kg Div\"],0)\n",
    "    aldi_df[\"Div_merged2\"] = np.where(aldi_df[\"Unit\"].str.contains(\"Liter\",\"ml\"), aldi_df[\"ML Div \"], aldi_df[\"Div_merged\"])\n",
    "    aldi_df[\"Div_merged3\"] = np.where(aldi_df[\"Unit\"].str.contains(\"ml\"), aldi_df[\"ML Div \"], aldi_df[\"Div_merged2\"])\n",
    "    aldi_df[\"Comparable Price\"] = np.where(aldi_df[\"Div_merged3\"] == 0, aldi_df[\"Price per\"], aldi_df[\"Div_merged3\"])\n",
    "    \n",
    "    \n",
    "    dropped = [\"Unnamed: 0\",\"1kg Div\",\"ML Div \",\"Div_merged\",\"Div_merged2\",\"Div_merged3\", \"Price per\"]\n",
    "    aldi_df = aldi_df.drop(columns=dropped)\n",
    "    \n",
    "    aldi_df[\"Unit\"] = aldi_df[\"Unit\"].str.replace(\"Preis pro 1 KG\",\"100 Gramm\").str.replace(\"Preis pro 100 Gramm\", \"100 Gramm\").str.replace(\"Preis pro 1 Liter\", \"100 ml\").str.replace(\"Preis pro 100 ml\", \"100 ml\")\n",
    "            \n",
    "    aldi_df[\"Packet Size\"] = aldi_df[\"Packet Size\"].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    return aldi_df\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in supermarket_csv:\n",
    "    frame = data_cleaner(i)\n",
    "    csvpath = \"cleaned_\" + i\n",
    "    frame.to_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner(supermarket_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaylsis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [\"supermarket_cleaned_csvs/cleaned_aldi_products.csv\",\n",
    "        \"supermarket_cleaned_csvs/cleaned_aldisued_products.csv\",\n",
    "        \"supermarket_cleaned_csvs/cleaned_edeka_products.csv\",\n",
    "        \"supermarket_cleaned_csvs/cleaned_kaufland_products.csv\",\n",
    "        \"supermarket_cleaned_csvs/cleaned_lidl_products.csv\",\n",
    "        \"supermarket_cleaned_csvs/cleaned_rewe_products.csv\"]\n",
    "         \n",
    "supermarket_names = [\"aldinord\",\"aldisued\",\"edeka\",\"kaufland\",\"lidl\",\"rewe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one frame with all the comparisons\n",
    "\n",
    "compare_frame = pd.DataFrame(columns=[\"Market1\",\"Market1Mean\",\"Market2\",\"Market2Mean\",\"RatioM1/M2\"])\n",
    "\n",
    "\n",
    "\n",
    "# Outer loop: Choose \"base\" file for merge\n",
    "\n",
    "# Loop for indices of clean\n",
    "for i in range(0,len(clean)):\n",
    "    df1 = pd.read_csv(clean[i])\n",
    "    \n",
    "    # Now, inner loop for all other files, same approach\n",
    "    for j in range(0,len(clean)):\n",
    "        df2 = pd.read_csv(clean[j])\n",
    "        \n",
    "        # Do the merge operation for all\n",
    "        df_merge = pd.merge(df1,df2,how='inner',on=\"Name\")\n",
    "        \n",
    "        \n",
    "        # Save as .csv. Give meaningful names. Since name list has same order as .csv file list, \n",
    "        # we can use the same indices\n",
    "        \n",
    "        name1 = supermarket_names[i]\n",
    "        name2 = supermarket_names[j]\n",
    "        df_merge.to_csv(f\"{name1}_{name2}.csv\")\n",
    "        \n",
    "        # Calculate means and ratio\n",
    "        \n",
    "        mean1 = df_merge[\"Comparable Price_x\"].mean()\n",
    "        mean2 = df_merge[\"Comparable Price_y\"].mean()        \n",
    "        \n",
    "        meanratio = mean1/mean2\n",
    "        \n",
    "        \n",
    "        # Add the row to the above frame\n",
    "        \n",
    "        comparerow = [name1, mean1, name2, mean2, meanratio]\n",
    "        \n",
    "        compare_frame.append(comparerow)\n",
    "        \n",
    "# save compare frame individually\n",
    "\n",
    "compare_frame.to_csv(\"compareframe.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
